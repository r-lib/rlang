---
title: "Lazy evaluation"
author: "Hadley Wickham"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Lazy evaluation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(lazyeval)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

Lazy evaluation is a principled way to do non-standard evaluation (NSE) in R. It is centered around formulas which capture an unevaluated expression and its associated environment. You use lazy evaluation by requiring the user to "quote" specially evaluated arguments with `~`, and then using the lazyeval package to compute on those formulas. Lazyeval also provides tools to eliminate the use of `~`, and convert unevaluated arguments to formulas. This does make programming with such functions a little harder, but it can be worth when every keystroke matters.

You should read this vignette if you want to program packages like dplyr and ggplot2[^1], or you want a principled way of working with delayed expressions in your own package.

[^1]: Currently neither ggplot2 nor dplyr actually use this technique since I've only just figured it out. But I'll be working hard to make sure all my packages are consistent in the near future.

My recommendations for how to do this have changed substantially over time. I am fairly confident they will not have to change again. This approach and accompanying tools allows you to solve a wide range of practical problems that were challenging previously and is rooted in [well-standing theory](http://repository.readscheme.org/ftp/papers/pepm99/bawden.pdf).

## What is non-standard evaluation?

As the name suggests, non-standard evaluation (NSE) breaks away from the standard evaluation (SE) rules in order to do something special. There are two common uses of NSE:

1.  Implement non-standard _scoping_, typically looking for values in a data 
    frame (or list) before the parent environment. This use of NSE includes 
    many functions in base R:

    ```{r}
    df <- data.frame(x = c(1, 5, 4, 2, 3), y = c(2, 1, 5, 4, 3))
    
    with(df, mean(x))
    subset(df, x == y)
    transform(df, z = x + y)
    ```

1.  Access the expression used to generate the value for a function argument. 
    This is often used to create informative labels or informative error 
    messages:
    
    ```{r, fig.width = 3, fig.height = 3}
    grid <- seq(0, 2 * pi, length = 100)
    plot(grid, sin(grid))
    ```

There are a handful of other uses that are often called [metaprogramming](http://adv-r.had.co.nz/Expressions.html). These all involve computing on the unevaluated code in some way. Here are two from base R:

1.  `bquote()` allows you to interpolate values into an expression:

    ```{r}
    x <- 10
    y <- 100
    f <- quote(`-`)
    
    bquote(.(f)(.(x), .(y)))
    ```

1.  `help()` and `library()` use non-standard evaluation so you that you
    don't have to put quotes around package names and help topics.

This document will focus on non-standard scoping because it's the most commonly used and most important, as described below. To access the expression associated with an argument, see `expr_find()` and `expr_label()`.

## Why use non-standard scoping?

Non-standard scoping is an important part of R because it makes it makes it possible to write functions that are tailored for interactive data exploration. These functions require less typing, at the expense of introducing some ambiguity and "magic". This is a good trade-off for interactive data exploration because you want to get ideas out of your head and into the computer as quickly as possible. Because you're working interactively you'll quickly spot if a function makes a bad guess.

There are three challenges to doing non-standard scoping correctly:

* To delay the computation of an expression you must capture both the 
  unevaluated expression and its environment. Formulas, described next,
  offer a convenient and explicit way of doing this. 
  
* Non-standard scoping introduces ambiguity that is useful for interactive
  exploration but needs some way to opt out for when programming. Lazyeval 
  provides `.data` and `.env` pronouns if you need to be explicit.

* You often want to mix evaluated and unevaluated components in a 
  single call. Lazyeval provides `f_interp()` which implements a full 
  quasi-quotation system with both unquote and unquote-splice components.

## Formulas

Formulas are a familiar tool from linear models, but they are actually a powerful general purpose tool. A formula captures two things: an unevaluated expression, and the environment in which it would be evaluated. The formula is a single character that allows you to say: "I want capture the meaning of this code, without evaluating it right away". For that reason, the formula can be thought of as general "quoting" operator.

Technically, a formula is an S3 class built on top of the language type (i.e. an unevaluated expression), with an attribute that stores the environment:

```{r}
f <- ~ x + y + z
typeof(f)
attributes(f)
```

Single-sided formulas have length two:

```{r}
length(f)
# The 1st element is always ~
f[[1]]
# The 2nd element is the quoted expression
f[[2]]
```

R also supports two-sided formulas like `y ~ x`. These are useful for modelling, but have a slightly different interface:

```{r}
g <- y ~ x + z
length(g)
# The 1st element is still ~
g[[1]]
# But now the 2nd element is the LHS
g[[2]]
# And the 3rd element is the RHS
g[[3]]
```

To avoid these differences, we'll only ever use singled-singled formulas for quoting. Lazyeval provides `f_rhs()` to access the call. It throws an error if the input is not of the expected type:

```{r, error = TRUE}
f_rhs(f)
f_rhs(g)
```

## Working with formulas

Once you have a formula, you need some tools to work with it. The goal of lazyeval is to provide those tools. The first tool is `f_eval()`; it evaluates the delayed expression captured by the formula.

```{r}
f <- ~ 1 + 2 + 3
f_eval(f)
```

This allows you to use a formula a way of delaying evaluation: you can capture the expression and its environment, and only evaluate it when you need to. Because the formula captures the enclosing environment, `f_eval()` is robust, and works even when the formula is used in a different place from where it was defined:

```{r}
foo <- function(x) {
  ~ 1000 + x
}
f <- foo(10)
f
f_eval(f)
```

It can be hard to see what's going on when looking at a formula because important values are stored in the associated environment. You can use `f_unwrap()` to replace names with their corresponding values:

```{r}
f
f_unwrap(f)
```

`f_eval()` has an optional second argument: a list (or data frame) that overrides values found in the environment. This allows you to implement non-standard scoping: referring to variables in a data frame as if they are variables in the environment:

```{r}
f_eval(~ mean(cyl), mtcars)
```

We can use these ideas to implement a simple version of `base::subset()` (if you're familiar with dplyr, `subset()` is very similar to `filter()`). The goal of `subset()` is to make it easy to select observations matching criteria defined by values of the variables. It has three advantages over `[`:

1.  If the criteria uses many variables, `subset()` is much more compact 
    because you don't need to repeat the name of the data frame each time.

1.  It drops rows where the condition evaluates to `NA` rather than filling
    them in with `NA`s
    
1.  It always returns a data frame so you don't need to remember to do
    `drop = FALSE` for single column data frames.

Here's a simple version of `subset.data.frame()` that uses the lazy eval approach:

```{R}
subset <- function(df, subset) {
  rows <- f_eval(subset, df)
  if (!is.logical(rows)) {
    stop("`subset` must be logical.", call. = FALSE)
  }
  
  rows <- rows & !is.na(rows)
  df[rows, , drop = FALSE]
}

df <- data.frame(x = 1:5, y = 5:1)
subset(df, ~ x <= 2)
subset(df, ~ x == y)
```

There's nothing new here: we require the user to provide a formula and then evaluate it with `f_eval()`. Once we have the logical vector, it only remains to replace `NA` with `FALSE`, and to perform the subsetting.

## `.data.` and `.env` pronouns

Imagine that you've written some code that looks like this:

```{r, eval = FALSE}
subset(march, ~ x > 100)
subset(april, ~ x > 50)
subset(june, ~ x > 45)
subset(july, ~ x > 17)
```

(This is admittedly a contrived example, but it illustrates all of the important issues you'll need to consider when writing more useful functions.)

Instead of continue to copy-and-paste your code, you decide to wrap up the common behaviour in a function: 

```{r}
threshold_x <- function(df, threshold) {
  subset(df, ~ x > threshold)
}
threshold_x(df, 3)
```

There are two ways that this function might fail:

1.  The data frame might not have a variable called `x`. This will fail unless
    there's a variable called `x` hanging around in the global environment:
    
    ```{r, error = TRUE}
    df2 <- data.frame(y = 5:1)
    
    # Throws an error
    threshold_x(df2, 3)
    
    # Silently gives the incorrect result!
    x <- 5
    threshold_x(df2, 3)
    ```
    
1.  The data frame might have a variable called `threshold`:

    ```{r}
    df3 <- data.frame(x = 1:5, y = 5:1, threshold = 4)
    threshold_x(df3, 3)
    ```

These failures are partiuclarly pernicious because they don't throw an error, but silently give an incorrect result. Both failures arise because `f_eval()` looks in two places for each name: the data frame and formula environment. To make this function more reliable, we need to be more explicit. `f_eval()` provides two pronouns to make this possible:

* `.data` is bound to the data frame.
* `.env` is bound to the formula environment.

(They both start with `.` to minimise the chances of overriding existing variables.)

We can use these pronouns to make `threshold_x()` more robust:

```{r, error = TRUE}
threshold_x <- function(df, threshold) {
  subset(df, ~ .data$x > .env$threshold)
}

threshold_x(df2, 3)
threshold_x(df3, 3)
```

Here `.env` is bound to the environment where `~` is evaluated, namely the inside of `threshold_x()`.

## Unquoting

The `threshold_x` function is not very useful because it's bound to a specific variable. It would be more powerful if we could vary the variable as well as the threshold. We can do that by taking an additional argument to specify which variable to use. One simple approach is to use a string and `[[`:

```{r}
threshold <- function(df, variable, threshold) {
  stopifnot(is.character(variable), length(variable) == 1)
  
  subset(df, ~ .data[[.env$variable]] > .env$threshold)
}
threshold(df, "x", 4)
```

This is a simple and robust solution, but only allows us to use an existing variable, not an arbitrary expression like `sqrt(x)`.

A more general solution is to allow the user to supply a formula, and use the special `uq()` function:

```{r}
threshold <- function(df, variable = ~x, threshold = 0) {
  subset(df, ~ uq(variable) > .env$threshold)
}

threshold(df, ~ x, 4)
threshold(df, ~ abs(x - y), 2)
```

How does this work? `f_eval()` lets you escape the quoting, and evaluate, or __unquote__, an expression. It's easiest to see what's happening by using the `f_interp()`  function which `f_eval()` uses to do the actual interpolation. The first argument to `uq()` is evaluated and inserted into the expression:
    
```{r}
f_interp(~ 1 + uq(1 + 1))

x <- 100
f_interp(~ 1 + uq(x))
```

If the first argument is a formula, then the right hand side is used:

```{r}
z <- ~ x + y
f_interp(~ mean( uq(z) ))
```
  
Of course this makes it difficult to insert literal formulas into a call:

```{r}
formula <- y ~ x
f_interp(~ lm(uq(formula), data = df))
```

So you can instead use `uqf()`:

```{r}
f_interp(~ lm(uqf(formula), data = df))
```

You typically won't call `f_interp()` directly as `f_eval()` calls it for you, but it's useful for understanding and debugging.

In this case it's the responsibility of the caller to avoid ambiguities:

```{r}
variable <- ~ .data$x + .env$y
f_interp(~ uq(f_rhs(variable)) > .env$threshold)
```

Unquoting is powerful, but it only allows you to modify a specific argument: it doesn't allow you to add an arbitrary number of arguments. To do that, you'll need "unquote-splice", or `uqs()`. The first argument to `uqs()` should be a list of arguments to be spliced into the call:

```{r}
extra_args <- list(na.rm = TRUE, trim = 0.9)
f_interp(~ mean(x, uqs(extra_args)))
```

## Eliminating the formulas

In some situations you might want to eliminate the formula altogether, and allow the user to type regular R expressions. Although I was once much enamoured with this approach (witness ggplot2, dplyr, ...), I now think that it should be used sparingly. Requiring explict quoting with `~`, leads to simpler code, and the explict quoting makes it clear to the user that something special will happen with the argument.

That said, lazyeval does allow you to eliminate the `~` if you really want to. You'll still need a function that works with formula, so give it the suffix `_`:

```{r}
subset_ <- function(df, subset) {
  rows <- f_eval(subset, df)
  if (!is.logical(rows)) {
    stop("`subset` must be logical.", call. = FALSE)
  }
  
  rows <- rows & !is.na(rows)
  df[rows, , drop = FALSE]
}
```

Once you have this version you can create a verison that doesn't need the explicit formula. The key is the use of `f_capture()` which takes an unevaluated argument and captures it as a formula:

```{r}
subset <- function(df, expr) {
  subset_(df, f_capture(expr))
}
subset(df, x == 1)
```

If you're familiar with `substitute()` you might expect the same drawbacks to apply. However, `f_capture()` is start enough to follow a chain of promises back to the original value, so, for example, this code works fine:

```{r}
scramble <- function(df) {
  df[sample(nrow(df)), , drop = FALSE]
}
subscramble <- function(df, expr) {
  scramble(subset(df, expr))
}
subscramble(df, x < 4)
```
